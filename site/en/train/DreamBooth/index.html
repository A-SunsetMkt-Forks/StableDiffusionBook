
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="一个介绍 Ai 绘画的 WIKI/A WIKI about Ai painting" name="description"/>
<meta content="StableDiffusionBook Team" name="author"/>
<link href="https://stable-diffusion-book.vercel.app/en/train/DreamBooth/" rel="canonical"/>
<link href="../../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.4.2, mkdocs-material-8.5.10" name="generator"/>
<title>DreamBooth - AiDraw</title>
<link href="../../../assets/stylesheets/main.472b142f.min.css" rel="stylesheet"/>
<link href="../../../assets/stylesheets/palette.08040f6c.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<meta content="website" property="og:type"/>
<meta content="DreamBooth - AiDraw" property="og:title"/>
<meta content="一个介绍 Ai 绘画的 WIKI/A WIKI about Ai painting" property="og:description"/>
<meta content="https://stable-diffusion-book.vercel.app/assets/images/social/train/DreamBooth.en.png" property="og:image"/>
<meta content="image/png" property="og:image:type"/>
<meta content="1200" property="og:image:width"/>
<meta content="630" property="og:image:height"/>
<meta content="https://stable-diffusion-book.vercel.app/en/train/DreamBooth/" property="og:url"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="DreamBooth - AiDraw" name="twitter:title"/>
<meta content="一个介绍 Ai 绘画的 WIKI/A WIKI about Ai painting" name="twitter:description"/>
<meta content="https://stable-diffusion-book.vercel.app/assets/images/social/train/DreamBooth.en.png" name="twitter:image"/>
<link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            
                .gscrollbar-fixer { padding-right: 15px; }
                .gdesc-inner { font-size: 0.75rem; }
                body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
                body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
                body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
                </style><script src="../../../assets/javascripts/glightbox.min.js"></script></head>
<body data-md-color-accent="" data-md-color-primary="white" data-md-color-scheme="default" dir="ltr">
<script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#dreambooth">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="AiDraw" class="md-header__button md-logo" data-md-component="logo" href="../../" title="AiDraw">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m20.71 4.63-1.34-1.34c-.37-.39-1.02-.39-1.41 0L9 12.25 11.75 15l8.96-8.96c.39-.39.39-1.04 0-1.41M7 14a3 3 0 0 0-3 3c0 1.31-1.16 2-2 2 .92 1.22 2.5 2 4 2a4 4 0 0 0 4-4 3 3 0 0 0-3-3Z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            AiDraw
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              DreamBooth
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="" data-md-color-media="" data-md-color-primary="white" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="" data-md-color-media="" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
</label>
</form>
<div class="md-header__option">
<div class="md-select">
<button aria-label="Select language" class="md-header__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m12.87 15.07-2.54-2.51.03-.03A17.52 17.52 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04M18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12m-2.62 7 1.62-4.33L19.12 17h-3.24Z"></path></svg>
</button>
<div class="md-select__inner">
<ul class="md-select__list">
<li class="md-select__item">
<a class="md-select__link" href="../../../train/DreamBooth/" hreflang="zh">
                    简体中文
                  </a>
</li>
<li class="md-select__item">
<a class="md-select__link" href="./" hreflang="en">
                    English
                  </a>
</li>
</ul>
</div>
</div>
</div>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/sudoskys/StableDiffusionBook" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-tabs__inner md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../">
        Start here
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../Perpare/">
        Before start
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../install/">
        Installation
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../model/">
        Configuration
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../paint/">
        Paint guide
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link md-tabs__link--active" href="../">
        Train Model
      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../landing/">
        References
      </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="AiDraw" class="md-nav__button md-logo" data-md-component="logo" href="../../" title="AiDraw">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m20.71 4.63-1.34-1.34c-.37-.39-1.02-.39-1.41 0L9 12.25 11.75 15l8.96-8.96c.39-.39.39-1.04 0-1.41M7 14a3 3 0 0 0-3 3c0 1.31-1.16 2-2 2 .92 1.22 2.5 2 4 2a4 4 0 0 0 4-4 3 3 0 0 0-3-3Z"></path></svg>
</a>
    AiDraw
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/sudoskys/StableDiffusionBook" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_1" id="__nav_1" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../">Start here</a>
<label for="__nav_1">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Start here" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_1">
<span class="md-nav__icon md-icon"></span>
          Start here
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../GettingStarted/news/">
        News
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../GettingStarted/term/">
        Terminology
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../GettingStarted/state/">
        Statement
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../GettingStarted/contributing/">
        Contribution Guide
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_2" id="__nav_2" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../Perpare/">Before start</a>
</div>
<nav aria-label="Before start" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
          Before start
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_3" id="__nav_3" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../install/">Installation</a>
<label for="__nav_3">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Installation" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
          Installation
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_3_2" id="__nav_3_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_2">
          Install SDWebUi
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Install SDWebUi" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_2">
<span class="md-nav__icon md-icon"></span>
          Install SDWebUi
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../install/WebUi/set/">
        Base guide
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../install/WebUi/launch/">
        LaunchWebUi
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../install/WebUi/Troubleshoot/">
        Troubleshoot
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4" id="__nav_4" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../model/">Configuration</a>
<label for="__nav_4">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Configuration" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
          Configuration
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../model/GPU/">
        Gpu Debuging
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model/NAI/">
        NAI leak report
      </a>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4_4" id="__nav_4_4" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../model/WebUi/">WebUi 配置</a>
<label for="__nav_4_4">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="WebUi 配置" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_4_4">
<span class="md-nav__icon md-icon"></span>
          WebUi 配置
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../model/WebUi/base/">
        Base guide
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model/WebUi/Troubleshoot/">
        Troubleshoot
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_5" id="__nav_5" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../paint/">Paint guide</a>
<label for="__nav_5">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Paint guide" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
          Paint guide
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_5_2" id="__nav_5_2" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../paint/WebUi/">Paint With WebUi</a>
<label for="__nav_5_2">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Paint With WebUi" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_2">
<span class="md-nav__icon md-icon"></span>
          Paint With WebUi
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../paint/WebUi/base/">
        Base guide
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../paint/WebUi/Advanced/">
        Advanced Guide
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../paint/WebUi/extension/">
        Extension
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_5_3" id="__nav_5_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_5_3">
          Application Guide
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Application Guide" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_3">
<span class="md-nav__icon md-icon"></span>
          Application Guide
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../paint/Practicalguide/">
        Practical Guide
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../paint/identify/">
        Identify
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" id="__nav_6" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../">Train Model</a>
<label for="__nav_6">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Train Model" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_6">
<span class="md-nav__icon md-icon"></span>
          Train Model
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../Textual/">
        Textual Inversion
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../Hypernetwork/">
        Hypernetwork
      </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          DreamBooth
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        DreamBooth
      </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#introduction">
    Introduction
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#select">
    Select
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#preparation">
    Preparation
  </a>
<nav aria-label="Preparation" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#model-transformation">
    Model transformation
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#datasets">
    Datasets
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#annotating-images">
    Annotating images
  </a>
<nav aria-label="Annotating images" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#augmentation">
    Augmentation
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#parameters">
    Parameters
  </a>
<nav aria-label="Parameters" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#parameter-setting">
    Parameter setting
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#script-parameters">
    Script parameters
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#key-notes">
    Key notes
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#explanation-instance-prompt-class-prompt">
    Explanation Instance Prompt / Class Prompt
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#about-v">
    About [V]
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#subject-images-class-images">
    Subject images / Class images
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#native-training">
    Native Training
  </a>
<nav aria-label="Native Training" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#resume-training-from-checkpoint">
    Resume training from checkpoint
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#train-text-encoder">
    Train Text Encoder
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#multiple-concept">
    Multiple Concept
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#aspect-ratio-bucketing">
    Aspect Ratio Bucketing
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#training">
    Training
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#using-on-windows">
    Using on Windows
  </a>
<nav aria-label="Using on Windows" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#interim-guide">
    Interim Guide
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#other">
    Other?
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#dreambooth-parameter-table">
    DreamBooth Parameter table
  </a>
<nav aria-label="DreamBooth Parameter table" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#arguments">
    Arguments
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../AestheticGradients/">
        AestheticGradients
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_7" id="__nav_7" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../landing/">References</a>
</div>
<nav aria-label="References" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_7">
<span class="md-nav__icon md-icon"></span>
          References
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#introduction">
    Introduction
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#select">
    Select
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#preparation">
    Preparation
  </a>
<nav aria-label="Preparation" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#model-transformation">
    Model transformation
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#datasets">
    Datasets
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#annotating-images">
    Annotating images
  </a>
<nav aria-label="Annotating images" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#augmentation">
    Augmentation
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#parameters">
    Parameters
  </a>
<nav aria-label="Parameters" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#parameter-setting">
    Parameter setting
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#script-parameters">
    Script parameters
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#key-notes">
    Key notes
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#explanation-instance-prompt-class-prompt">
    Explanation Instance Prompt / Class Prompt
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#about-v">
    About [V]
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#subject-images-class-images">
    Subject images / Class images
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#native-training">
    Native Training
  </a>
<nav aria-label="Native Training" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#resume-training-from-checkpoint">
    Resume training from checkpoint
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#train-text-encoder">
    Train Text Encoder
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#multiple-concept">
    Multiple Concept
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#aspect-ratio-bucketing">
    Aspect Ratio Bucketing
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#training">
    Training
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#using-on-windows">
    Using on Windows
  </a>
<nav aria-label="Using on Windows" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#interim-guide">
    Interim Guide
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#other">
    Other?
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#dreambooth-parameter-table">
    DreamBooth Parameter table
  </a>
<nav aria-label="DreamBooth Parameter table" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#arguments">
    Arguments
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/sudoskys/StableDiffusionBook/edit/main/docs/train/DreamBooth.en.md" title="Edit this page">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"></path></svg>
</a>
<h1 id="dreambooth">DreamBooth</h1>
<blockquote>
<p>TODO/HELP Audit</p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p><a href="https://dreambooth.github.io/">DreamBooth</a> is a method for customising a personalised TextToImage diffusion model. Excellent results can be obtained with only a small amount of training data.</p>
<p>Dreambooth is based on <a href="https://imagen.research.google/">Imagen</a> and can be used by simply exporting the model as a ckpt, which can then be loaded into various UIs.</p>
<p>However, neither the Imagen model nor the pre-trained weights are available. So the original Dreambooth was not suitable for stable diffusion. But later <a href="https://github.com/ShivamShrirao/diffusers">diffusers</a> implements <a href="https://github.com/huggingface/diffusers/tree/main/examples/dreambooth">Dreambooth</a> and is fully adapted to stable diffusion.</p>
<blockquote>
<p>Diffusers provides pre-trained diffusion models across multiple modalities (e.g. visual and audio) and is supported as a modular toolbox for diffusion model inference and training.</p>
</blockquote>
<p>This section uses a version of the <a href="https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth">diffusers</a> branch of Shivam Shirao, with a configuration derived from <a href="https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth">ShivamShrirao/diffusers</a>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="https://dreambooth.github.io/DreamBooth_files/system.png"><img alt="DreamBooth_files" src="https://dreambooth.github.io/DreamBooth_files/system.png"/></a></p>
<blockquote>
<p>by https://dreambooth.github.io/</p>
</blockquote>
<h2 id="select">Select</h2>
<p>Windows systems require a minimum of 16GB of video memory, Linux systems require a minimum of 8GB of video memory</p>
<ul>
<li>
<p><a href="https://github.com/crosstyan/dreambooth-scripts-for-autodl">DreamBooth version</a> for AutoDl</p>
</li>
<li>
<p>Package image](https://github.com/CrazyBoyM/dreambooth-for-diffusion) for AutoDl with the name <code>dreambooth-for-diffusion</code></p>
</li>
<li>
<p><a href="https://github.com/d8ahazard/sd_dreambooth_extension">Plugin</a> for WebUi</p>
</li>
<li>
<p>For Colab <a href="https://colab.research.google.com/drive/17yM4mlPVOFdJE_81oWBz5mXH9cxvhmz8">Nyanko Lepsoni's Colab notebook</a></p>
</li>
<li>
<p>Colab notebook from <a href="https://colab.research.google.com/drive/1C1vVZ59S4kWfL7jIsczyLpmxbD4cOA-k">RcINS</a> for Colab</p>
</li>
</ul>
<blockquote>
<p>Colab notebooks from <a href="https://t.me/StableDiffusion_CN/196744">Community Top</a></p>
</blockquote>
<h2 id="preparation">Preparation</h2>
<p>If you choose to use the AutoDl image, you will need to move the dreambooth-for- diffusion folder to autodl-tmp (the data disk) and make sure that the path (the string before the #) is dreambooth-for-diffusion, as detailed in the <a href="https://zhuanlan.zhihu.com/p/584736850">tutorial</a> The details are illustrated in the tutorial [].</p>
<p>Enable prior_preservation to start DreamBooth training. the lower the prior_loss_weight the harder it is to fit, but also the harder it is to learn. <a href="[dreambooth-training-guide](https://github.com/nitrosocke/dreambooth-training-guide)">4^</a> </p>
<h3 id="model-transformation">Model transformation</h3>
<p>The following type of command is available in each notebook or mirror and serves to convert the ckpt model to diffusers weight format for training.</p>
<ul>
<li>Pre-training example</li>
</ul>
<div class="highlight"><pre><span></span><code>    python diffusers<span class="se">\s</span>cripts<span class="se">\c</span>onvert_original_stable_diffusion_to_diffusers.py --checkpoint_path model.ckpt --original_config_file v1-inference.yaml --scheduler_type ddim --dump_path models/diffusers_model
</code></pre></div>
<ul>
<li>Post-training example</li>
</ul>
<p>The training is also packaged and converted to ckpt, which can be used in various UI's.</p>
<div class="highlight"><pre><span></span><code>    python diffusers<span class="se">\s</span>cripts<span class="se">\c</span>onvert_diffusers_to_original_stable_diffusion.py --model_path models/resultModel --checkpoint_path result.ckpt --half
</code></pre></div>
<h3 id="datasets">Datasets</h3>
<p>The creation of the dataset is the most important part of getting good, consistent results in Dreambooth training.</p>
<ul>
<li>Content requirements</li>
</ul>
<p>Always use high quality samples, content such as motion blur or low resolution can be trained into the model and affect the quality of the work.</p>
<p>When training for a specific style, pick samples that have good consistency. Ideally, only pick images of the artist you are training. Avoid fan art or anything with a different style, unless your goal is something like stylistic fusion.</p>
<p>For themes, samples with black or white backgrounds are extremely helpful.</p>
<blockquote>
<p>Transparent backgrounds work too, but sometimes they leave a white outline around the subject, so I don't recommend using them at the moment.</p>
</blockquote>
<p>If you need to make your Dreambooth model more varied, try to use different environments, lighting, hair styles, expressions, poses, angles and distances from the subject.</p>
<p>Be sure to include images with normal backgrounds (e.g. of the object in a scene). Using only images with simple backgrounds will be less effective.</p>
<p>Avoid having hands stuck to the head in your renders by removing all images where the hands are too close to or touching the head.</p>
<p>If you need to avoid a fisheye lens effect in your renders, remove all selfie images.</p>
<p>To avoid unnatural over-blurring, make sure the image does not contain a false heavy depth of field or vignetting.</p>
<ul>
<li>Adjustment</li>
</ul>
<p>Once you have collected the photos for your dataset, crop and resize all images to a 512x512 square (you can batch crop using the <a href="https://www.onlinephotosoft.com/birme/">BIRME</a> online tool) and remove any watermarks, logos, people/limb cut off by the edges of the image, or anything else you don't want to be trained on. Save the images in PNG format and place all images in the train folder.</p>
<h3 id="annotating-images">Annotating images</h3>
<p>You can annotate manually or automatically using clip or deepdanbooru.</p>
<p>We recommend using <a href="https://github.com/crosstyan/blip_helper">crosstyan/blip_helper</a> to label your images. Or use <a href="https://github.com/KichangKim/DeepDanbooru">DeepDanbooru</a> and <a href="https://github.com/salesforce/BLIP">BLIP</a></p>
<p>If you use AutoDl's mirrors, you can use the built-in <a href="https://github.com/CrazyBoyM/dreambooth-for-diffusion/blob/main/tools/label_images.py">label_images.py</a> for labelling.</p>
<h4 id="augmentation">Augmentation</h4>
<ul>
<li>There are many ways to manipulate data: the most common are inversion, rotation, brightness and cropping.</li>
</ul>
<blockquote>
<p>It may be useful to break up the image, or to crop the background/headers etc. separately.</p>
</blockquote>
<h3 id="parameters">Parameters</h3>
<p>First let's look at an example from the <a href="https://colab.research.google.com/drive/1C1vVZ59S4kWfL7jIsczyLpmxbD4cOA-k">RcINS' Colab notebook</a> notebook.</p>
<h4 id="parameter-setting">Parameter setting</h4>
<div class="highlight"><pre><span></span><code><span class="nv">INSTANCE_PROMPT</span> <span class="o">=</span> <span class="s2">"masterpiece, best quality, sks 1girl"</span>

<span class="c1"># images of the subject 数据集的图像</span>
<span class="nv">INSTANCE_DIR</span> <span class="o">=</span> <span class="s2">"/content/instance-images"</span>

<span class="c1"># Class set </span>
<span class="nv">CLASS_PROMPT</span> <span class="o">=</span> <span class="s2">"masterpiece, best quality, 1girl"</span> 
<span class="nv">CLASS_NEGATIVE_PROMPT</span> <span class="o">=</span> <span class="s2">"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry"</span> 
<span class="nv">CLASS_DIR</span> <span class="o">=</span> <span class="s2">"/content/class-images"</span> 
<span class="nv">NUM_CLASS_IMAGES</span> <span class="o">=</span> <span class="m">100</span> 

<span class="c1"># markdown Prompt for saving samples.</span>
<span class="nv">SAVE_SAMPLE_PROMPT</span> <span class="o">=</span> <span class="s2">"masterpiece, best quality, sks 1girl, looking at viewer"</span>
<span class="nv">SAVE_SAMPLE_NEGATIVE_PROMPT</span> <span class="o">=</span> <span class="s2">"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry"</span>
</code></pre></div>
<h4 id="script-parameters">Script parameters</h4>
<div class="highlight"><pre><span></span><code><span class="nv">wandb_arg</span> <span class="o">=</span> <span class="s2">"--wandb"</span> <span class="k">if</span> WANDB_KEY !<span class="o">=</span> <span class="s2">""</span> <span class="k">else</span> <span class="s2">""</span>
<span class="nv">scale_lr_arg</span> <span class="o">=</span> <span class="s2">"--scale_lr"</span> <span class="k">if</span> SCALE_LR <span class="k">else</span> <span class="s2">""</span>
<span class="nv">ppl_arg</span> <span class="o">=</span> f<span class="s2">"--with_prior_preservation --prior_loss_weight={PRIOR_LOSS_WEIGHT}"</span> <span class="k">if</span> PRIOR_PRESERVATION <span class="k">else</span> <span class="s2">""</span>
<span class="nv">read_prompt_arg</span> <span class="o">=</span> f<span class="s2">"--read_prompt_from_txt {READ_PROMPT_FROM_TXT}"</span> <span class="k">if</span> READ_PROMPT_FROM_TXT !<span class="o">=</span> <span class="s2">"no"</span> <span class="k">else</span> <span class="s2">""</span>
<span class="nv">arb_arg</span> <span class="o">=</span> <span class="s2">"--use_aspect_ratio_bucket --debug_arb"</span> <span class="k">if</span> ASPECT_RATIO_BUCKETING <span class="k">else</span> <span class="s2">""</span>

accelerate launch <span class="nv">$TRAINER</span> <span class="se">\</span>
  --instance_data_dir <span class="s2">"{INSTANCE_DIR}"</span> <span class="se">\</span>
  --instance_prompt <span class="s2">"{INSTANCE_PROMPT}"</span> <span class="se">\</span>
  --pretrained_model_name_or_path <span class="s2">"{MODEL_NAME}"</span> <span class="se">\</span>
  --pretrained_vae_name_or_path <span class="s2">"{MODEL_NAME}/vae"</span> <span class="se">\</span>
  --output_dir <span class="s2">"{OUTPUT_DIR}"</span> <span class="se">\</span>
  --seed<span class="o">=</span><span class="nv">$SEED</span> <span class="se">\</span>
  --resolution<span class="o">=</span><span class="nv">$RESOLUTION</span> <span class="se">\</span>
  --optimizer <span class="s2">"{OPTIMIZER}"</span> <span class="se">\</span>
  --train_batch_size<span class="o">=</span><span class="nv">$TRAIN_BATCH_SIZE</span> <span class="se">\</span>
  --learning_rate<span class="o">=</span><span class="nv">$LEARNING_RATE</span> <span class="se">\</span>
  --lr_scheduler<span class="o">=</span><span class="nv">$LR_SCHEDULER</span> <span class="se">\</span>
  --lr_warmup_steps<span class="o">=</span><span class="nv">$LR_WARMUP_STEPS</span> <span class="se">\</span>
  --lr_cycles<span class="o">=</span><span class="nv">$LR_CYCLES</span> <span class="se">\</span>
  --last_epoch<span class="o">=</span><span class="nv">$LAST_EPOCH</span> <span class="se">\</span>
  --max_train_steps<span class="o">=</span><span class="nv">$MAX_TRAIN_STEPS</span> <span class="se">\</span>
  --save_interval<span class="o">=</span><span class="nv">$SAVE_INTERVAL</span> <span class="se">\</span>
  --class_data_dir <span class="s2">"{CLASS_DIR}"</span> <span class="se">\</span>
  --class_prompt <span class="s2">"{CLASS_PROMPT}"</span> --class_negative_prompt <span class="s2">"{CLASS_NEGATIVE_PROMPT}"</span> <span class="se">\</span>
  --num_class_images<span class="o">=</span><span class="nv">$NUM_CLASS_IMAGES</span> <span class="se">\</span>
  --save_sample_prompt <span class="s2">"{SAVE_SAMPLE_PROMPT}"</span> --save_sample_negative_prompt <span class="s2">"{SAVE_SAMPLE_NEGATIVE_PROMPT}"</span> <span class="se">\</span>
  --n_save_sample<span class="o">=</span><span class="nv">$SAMPLE_N</span> <span class="se">\</span>
  --infer_batch_size<span class="o">=</span><span class="nv">$INFER_BATCH_SIZE</span> <span class="se">\</span>
  --infer_steps<span class="o">=</span><span class="nv">$INFER_STEPS</span> <span class="se">\</span>
  --guidance_scale<span class="o">=</span><span class="nv">$GUIDANCE_SCALE</span> <span class="se">\</span>
  --gradient_accumulation_steps<span class="o">=</span><span class="nv">$GRADIENT_ACCUMULATION_STEPS</span> <span class="se">\</span>
  --gradient_checkpointing <span class="se">\</span>
  --save_unet_half <span class="se">\</span>
  --mixed_precision <span class="s2">"{MIXED_PRECISION}"</span> <span class="se">\</span>
  --clip_skip<span class="o">=</span><span class="nv">$CLIP_SKIP</span> <span class="se">\</span>
  <span class="nv">$wandb_arg</span> <span class="nv">$scale_lr_arg</span> <span class="nv">$ppl_arg</span> <span class="nv">$read_prompt_arg</span> <span class="nv">$arb_arg</span>

<span class="c1"># disabled: --not_cache_latents </span>
</code></pre></div>
<ul>
<li>Instance Image </li>
</ul>
<p>The target dataset you are training on.</p>
<ul>
<li>Class/Regularization </li>
</ul>
<p>Don't be overly concerned, the Image should be an auto-generated image that is used to detect the AI's a priori knowledge. You should not put any non-AI generated images. If you are sure about this you should go for Native Training. (Adulteration of same-style images in clas image was an early discovery detour and is no longer encouraged)</p>
<ul>
<li>learning_rate</li>
</ul>
<p>DreamBooth itself has a very strong copy and paste effect. Use class/regularization to suppress this effect.</p>
<ul>
<li>use_txt_as_label</li>
</ul>
<p>Whether or not to read a txt file with the same name as the image as a label; this option ignores the <code>instance_prompt</code> argument. Usually used in style training.</p>
<ul>
<li>center_crop </li>
</ul>
<p>The script comes with an option to crop the image, it is recommended that you crop it to a square.</p>
<ul>
<li>resolution </li>
</ul>
<p>The resolution of the image (usually 512), defining this parameter will scale the image.</p>
<ul>
<li>max_train_steps</li>
</ul>
<p>The maximum number of training steps, usually 1000, if your dataset is large then increase the value.</p>
<p>The number of training steps is generally chosen such that training steps = (reference image x 100).</p>
<ul>
<li>save_model_every_n_steps</li>
</ul>
<p>Save_model_every_steps is a good way to see the intermediate training results and find the best model, it can also be used to restore the last training results from a checkpoint (colab laptop users are advised to mount to the cloud).</p>
<ul>
<li>lr_scheduler</li>
</ul>
<p>learning rate moderator, optionally <code>constant, linear, cosine, cosine_with_restarts, cosine_with_hard_restarts</code></p>
<ul>
<li>with_prior_preservation</li>
</ul>
<p>Disable to enable Native Training</p>
<h4 id="key-notes">Key notes</h4>
<p>Generally the training style removes the <code>-with_prior_preservation</code> parameter, but it is necessary to prepare prompts for each image to be trained, i.e. enable <code>-use_txt_as_label</code> to read a txt file with the same name as the image as a label (i.e. put another txt with the same name), this option ignores the content passed in by the instance_prompt parameter.
This is reflected in <a href="https://github.com/CrazyBoyM/dreambooth-for-diffusion/blob/main/train_style.sh">train_style.sh</a>.</p>
<p>Generally training specific objects/characters is done with <code>-with_prior_preservation</code> enabled using only the single label <code>-instance_prompt</code>, which requires prior knowledge of Ai, and with <code>-class_prompt</code> <code>-class_data_dir</code> parameter enabled, <strong>class dir is automatically generated</strong> and is cleared once per retraining. Embodied in <a href="https://github.com/CrazyBoyM/dreambooth-for-diffusion/blob/main/train_object.sh">train_object.sh</a>.</p>
<p>Here are some scattered explanations.</p>
<h4 id="explanation-instance-prompt-class-prompt">Explanation Instance Prompt / Class Prompt</h4>
<ul>
<li>Instance Prompt </li>
</ul>
<p>The default implementation is a globally shared prompt, which may work for a few shots, i.e. DreamBooth (original paper method).</p>
<p>However, when you have more training targets, this parameter no longer applies and you can turn on the <code>combine_prompt_from_txt</code> option to have a prompt (usually a txt) for each instance, i.e. DreamBooth (alternative method). The Instance Prompt should contain a unique identifier [V]</p>
<ul>
<li>Class Prompt</li>
</ul>
<p>Don't worry too much about it, it's automatically generated, it's recommended to generate it separately from other reasoning front-ends that support CLIP SKIP 2 and drop it into the class img set, which can also be read from a separate txt.</p>
<ul>
<li>Note</li>
</ul>
<p>class prompt will be used to generate a class of images, treated as something like <code>photo of a person</code></p>
<p>instance prompt will be processed as something like <code>photo of a cute person</code>.</p>
<ul>
<li>Examples</li>
</ul>
<p><em>Example of a training character</em>
* Instance prompt: <code>masterpiece, best quality, sks 1girl, aqua eyes, aqua hair</code>
* Class prompt: <code>masterpiece, best quality, 1girl, aqua eyes, aqua hair</code></p>
<p><em>Example of a training style</em>
* Instance prompt: <code>1girl, by sks</code>
* Class prompt: <code>1girl</code></p>
<h4 id="about-v">About [V]</h4>
<table>
<thead>
<tr>
<th>What your training set is about</th>
<th>Instance prompt must contain</th>
<th>Class prompt should describe</th>
</tr>
</thead>
<tbody>
<tr>
<td>A object/person</td>
<td><code>[V]</code></td>
<td>The object's type and/or characteristics</td>
</tr>
<tr>
<td>A artist's style</td>
<td><code>by [V]</code></td>
<td>The common characteristics of the training set</td>
</tr>
</tbody>
</table>
<p>[V] is a token in the CLIP vocabulary and has no meaning for the model.</p>
<p>Suppose the character you want to train is called <code>[N]</code> (e.g. <code>Mr. Balabalabala</code>) , you should not use <code>[N]</code> (<code>Mr. Balabalabala</code>) directly as a representative feature word.</p>
<p>It is recommended to use the word <code>[V]</code> (e.g. <code>bala</code>) that is present in <a href="https://huggingface.co/openai/clip-vit-large-patch14/raw/main/vocab.json">this glossary</a> but has no corresponding concept or where the corresponding concept is not obvious.</p>
<p>Long names are likely to be separated into multiple tokens and will not have the desired effect. The separation of tokens can be verified at <a href="https://novelai.net/tokenizer">NovelAI Tokenizer</a>.</p>
<p>The final hint representing [V] will carry the new things learned by the model and you will be able to use the [V] you set when generating it.</p>
<blockquote>
<p>Note: The example word <code>sks</code> used in the original paper is the same as the real firearm <a href="https://en.wikipedia.org/wiki/SKS">SKS</a> and is not a suitable word to be used. However, if you are sufficiently trained you may be able to override its effects.</p>
</blockquote>
<h4 id="subject-images-class-images">Subject images / Class images</h4>
<p>Introduction from <a href="[good_dreambooth_formula](https://www.reddit.com/r/StableDiffusion/comments/ybxv7h/good_dreambooth_formula/)">2^</a></p>
<p>Subject images (or instance images as you see them in your notebook) are the images you want to train on, so if you want your own looking model, you can take 20 to 40 of your own images and enter those. The instance name is a unique identifier that will indicate the trained object in the prompt, personally I use "namelastname", most notebooks use "sks" but it is best to change it.</p>
<p>You are in effect telling the AI to introduce you to the database, to do this you select a class, i.e. the class that best suits the person you are training, for people it is common to use "person", "man"/"woman" etc.</p>
<p>The purpose of using Class images in training is to <strong>prevent features of objects from "bleeding over" into other objects of the same Class</strong>. Without Class images as a reference point, the AI will merge your face with other faces that appear in the Class. <strong>Your face will infiltrate the other faces generated by the model. </strong></p>
<p>DreamBooth can start training without Class images, just disable <code>-with_prior_preservation</code> to enable Native Training.</p>
<h3 id="native-training">Native Training</h3>
<p>Unlike DreamBooth, Native Training uses your training set to train directly and does not require a Class Image.</p>
<p>Turning off the <code>-prior_preservation</code> option (i.e. the <code>-with_prior_preservation</code> parameter) to start training natively is the recommended way to train images.</p>
<p>There is no Instance/Class Image distinction in this training, all images will be used for training. However, you will need to prepare an Instance Prompt for each image, with the same file name as the traditional hypernetwork, usually txt.</p>
<div class="admonition tip">
<p class="admonition-title">About this Txt</p>
<p>For each image in the dataset ([X].png/[X].jpg), put a [X].txt with the corresponding hint, then set <code>READ_PROMPT_FROM_TXT</code> (<code>--use_txt_as_label</code>). The prompts read from txt [PX] will be inserted into the prompt [P] you set in the training parameters. By default, it is inserted as [PX][P]. With Variable Prompts enabled and Prior Preservation Loss (PRIOR_PRESERVATION) disabled, the training process is effectively equivalent to standard fine-tuning. Native Training requires a large dataset, but the amount varies, and is in the range [100, 10000], so more is better. (manual selection is recommended)</p>
</div>
<h4 id="resume-training-from-checkpoint">Resume training from checkpoint</h4>
<p>The MODEL_NAME of the parameter is changed to the position of the last model.</p>
<p>If you use <code>CLASS_DIR</code>, you don't need to clear it because the subject is the same, but vice versa.</p>
<h4 id="train-text-encoder">Train Text Encoder</h4>
<p>The use of <code>-train_text_encoder</code> in the corresponding example is not recommended. With <code>--train_text_encoder</code>, Dreambooth training trains additional text encoders, making it impossible to generalize the prompt between different models.</p>
<p>There is a metaphysical argument that this should be turned off after a certain percentage/epoch/step of training has been reached to prevent overplaying.</p>
<ul>
<li>The instance prompt you write at the beginning should be long and summarise your training goal (but not so long that it covers your usual words) (e.g. girl I would replace with woman, 1boy with male)</li>
<li>Firstly, the text prompt should be read in as a mail. Because too many words are distracting, the effect is not obvious.</li>
<li>Secondly, the instance prompt cannot be filled in with just one <code>[V]</code>, otherwise the word will be wasted too.</li>
<li>Try a big fire burst</li>
</ul>
<p>If you refine the call, add the word you trained for instance prompt, depending on how much flavour you want.</p>
<p>Maybe it's also effective when training characters.</p>
<h4 id="multiple-concept">Multiple Concept</h4>
<p>With DreamBooth it is possible to train multiple concepts/characters/actions/objects with the <code>--concept_list</code> argument in the example. However, if you train two characters, you cannot infer that they are both present at the same time, and their features will be mixed.</p>
<p>If you check the <code>--concept_list</code> parameter with other versions of the DreamBooth training method, you can read in a similar <code>json</code> file.</p>
<ul>
<li>concepts_list.json</li>
</ul>
<div class="highlight"><pre><span></span><code># You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.

concepts_list = [
    {
        "instance_prompt":      "photo of zwx dog",
        "class_prompt":         "photo of a dog",
        "instance_data_dir":    "/content/data/zwx",
        "class_data_dir":       "/content/data/dog"
    },
#     {
#         "instance_prompt":      "photo of ukj person",
#         "class_prompt":         "photo of a person",
#         "instance_data_dir":    "/content/data/ukj",
#         "class_data_dir":       "/content/data/person"
#     }
]

# `class_data_dir` contains regularization images
</code></pre></div>
<h4 id="aspect-ratio-bucketing">Aspect Ratio Bucketing</h4>
<p>Corresponds to <code>--use_aspect_ratio_bucket</code> in the argument above. If you need to use it in Colab, <code>-aspect_ratio_bucket</code> is set to <code>enable: true</code>.</p>
<p><a href="https://github.com/NovelAI/novelai-aspect-ratio-bucketing">Aspect Ratio Bucketing</a> is known as ARB, the original training only works with <code>1:1</code> images, enabling ARB makes it possible to train non-<code>1:1</code> images, but not at any scale, and images not in the bucket will be cropped.</p>
<div class="highlight"><pre><span></span><code>[[ 256 1024], [ 320 1024], [ 384 1024], [ 384  960], [ 384  896], [ 448  832], [ 512  768], [ 512  704], [ 512  512], [ 576  640], [ 640  576], [ 704  512], [ 768  512], [ 832  448], [ 896  384], [ 960  384], [1024  384], [1024  320], [1024  256]]
</code></pre></div>
<blockquote>
<p>ARB is not compatible with DreamBooth and is only recommended for Native Training.</p>
</blockquote>
<p>For more, see <a href="https://wandb.ai/psuraj/dreambooth/reports/Dreambooth-training-analysis--VmlldzoyNzk0NDc3">Analysis of experiments using Dreambooth to train stable diffusion</a></p>
<h3 id="training">Training</h3>
<p>Follow the notebook steps or instructions to train, Colab users should be careful to mount the disk to prevent disconnection.</p>
<p>After training, convert the diffusers weights to a ckpt file before you can use them! Use a file like <code>diffusers2ckpt.py</code>.</p>
<blockquote>
<p>Some scripts provide a <code>-half</code> parameter to save the float16 half-precision model, which halves the size of the weights file (about 2g), but the effect is essentially the same.</p>
</blockquote>
<p>WebUi users copy the trained <code>.ckpt</code> file into the <code>models\Stable-diffusion</code> directory of webui and switch between models in the top left corner of webui to use it.</p>
<p>To use it, enter the flags you specified before in <code>prompt</code> (e.g. <balabala>) to get the ai to add what you want to the generated image.</balabala></p>
<p>The effect of the model will depend on the test image.</p>
<h2 id="using-on-windows">Using on Windows</h2>
<p>The content here is suitable for who want to train on Windows systems.</p>
<p>From [^18]</p>
<p><code>Dreambooth</code> on Windows can use an optimized version of ShivamShrirao to save VRAM, <a href="https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth">diffusers/examples/dreambooth</a> by ShivamShrirao.</p>
<p>The deployment method used for <code>Linux</code> cannot be used directly on windows due to the associated library, and for the same reason the minimum 9.9G of RAM required for this optimised version on colab should be slightly higher on windows, so it is recommended to use <strong>at least a 12G VRAM</strong> GPU.</p>
<blockquote>
<p>i has only tested this on machines with 16G of RAM, 12G of RAM is theoretically possible.</p>
</blockquote>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When modifying or overwriting files in the original repository, please make a backup.</p>
</div>
<p>Prepare your environment with <code>Git</code>, <code>Python</code>, <code>MiniConda</code> (or <code>MiniConda</code>).</p>
<p>The following steps work in python 3.8, windows 10 22H2, other environments have not been tested</p>
<p><strong>Create a working directory in which to build the venv virtual environment for python 3.8</strong></p>
<div class="highlight"><pre><span></span><code>    python -m venv --system-site-packages venv_dbwin
    venv_dbwin<span class="se">\S</span>cripts<span class="se">\a</span>ctivate
    python.exe -m pip install --upgrade pip
</code></pre></div>
<p>Clone the optimized version of dreambooth for ShivamShrirao into the working directory and install the relevant dependencies (using <a href="https://github.com/huggingface/diffusers/tree/7465397f33d5de75dcccc155e3fb9a232fcbb0a0">build version</a>, which may not be supported in subsequent versions) method)</p>
<div class="highlight"><pre><span></span><code>git clone https://github.com/ShivamShrirao/diffusers
<span class="nb">cd</span> diffusers
pip install -e .
<span class="nb">cd</span> examples<span class="se">\d</span>reambooth
pip install -U -r requirements.txt
pip install OmegaConf
pip install pytorch_lightning
pip install einops
pip install <span class="nv">bitsandbytes</span><span class="o">==</span><span class="m">0</span>.34
</code></pre></div>
<p>Once this is done, you will need to implement bitsandbytes support on windows according to the methods in <a href="https://github.com/TimDettmers/bitsandbytes/issues/30#issuecomment-1257676341">this issue</a></p>
<p>Manually copy the <code>libbitsandbytes_cuda116.dll</code> file from <a href="https://github.com/DeXtmL/bitsandbytes-win-prebuilt">this repository</a> to the <code>venv_diffusers\Lib\site-packages\bitsandbytes</code> in the working directory next to <code>libbitsandbytes_cuda116.so</code>.</p>
<p>Then change the script to apply it, either manually or, for convenience, by downloading the following file to replace it.</p>
<p>Overwrite <code>cextension.py</code> - https://pastebin.com/jjgxuh8V with the <code>venv_diffusers\Lib\site-packages\bitsandbytes</code> directory.</p>
<p>Overwrite <code>main.py</code> - https://pastebin.com/BsEzpdpw to the <code>venv_diffusers\Lib\site-packages\bitsandbytes\cuda_setup</code> directory.</p>
<p><strong>Set PyTorch &amp; Torchvision</strong></p>
<p><code>pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116</code></p>
<p><strong>Test</strong></p>
<div class="highlight"><pre><span></span><code>    python
    &gt;&gt;&gt;import bitsandbytes
</code></pre></div>
<p>If no errors are reported the installation is successful (<code>Ctrl + Z</code> and enter to exit python).</p>
<h4 id="interim-guide">Interim Guide</h4>
<p><strong>Set up accelerate</strong></p>
<div class="highlight"><pre><span></span><code>    accelerate config

    In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0
    Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU [4] MPS): 0
    Do you want to run your training on CPU only (even if a GPU is available)? [yes/NO]:NO
    Do you want to use DeepSpeed? [yes/NO]:NO
    Do you wish to use FP16 or BF16 (mixed precision)? [NO/fp16/bf16]: fp16
</code></pre></div>
<p>Follow the setting options above, i.e. (0, 0, NO, NO, fp16).</p>
<p><strong>Videosity Occupancy Optimization</strong></p>
<p>Since xformers are not available, use optimization method in the
 <a href="https://github.com/lucidrains/memory-efficient-attention-pytorch/blob/main/memory_efficient_attention_pytorch/flash_attention.py">file</a> instead.</p>
<p>将 attention.py - https://pastebin.com/nmwTrGB9 覆盖到 <code>diffusers\src\diffusers\models</code> 目录。</p>
<p><strong>The environment is now configured and ready for training.</strong></p>
<div class="highlight"><pre><span></span><code>        accelerate launch --num_cpu_threads_per_process <span class="m">8</span> diffusers/examples/dreambooth/train_dreambooth.py --pretrained_model_name_or_path<span class="o">=</span>models/diffusers_model --pretrained_vae_name_or_path<span class="o">=</span>models/diffusers_model/vae --output_dir<span class="o">=</span>models --concepts_list<span class="o">=</span><span class="s2">"concepts_list.json"</span> --with_prior_preservation --prior_loss_weight<span class="o">=</span><span class="m">1</span>.0 --seed<span class="o">=</span><span class="m">1337</span> --resolution<span class="o">=</span><span class="m">512</span> --mixed_precision<span class="o">=</span><span class="s2">"fp16"</span> --lr_scheduler<span class="o">=</span><span class="s2">"constant"</span> --use_8bit_adam --gradient_accumulation_steps<span class="o">=</span><span class="m">1</span> --train_batch_size<span class="o">=</span><span class="m">1</span> --max_train_steps<span class="o">=</span><span class="m">800</span> --save_interval<span class="o">=</span><span class="m">10000</span> --learning_rate<span class="o">=</span>1e-6 --num_class_images<span class="o">=</span><span class="m">100</span> --lr_warmup_steps<span class="o">=</span><span class="m">0</span> --gradient_checkpointing
</code></pre></div>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="https://user-images.githubusercontent.com/44570237/198906326-21b4f779-f870-4012-84c1-d5ac1dae0411.png"><img alt="image" src="https://user-images.githubusercontent.com/44570237/198906326-21b4f779-f870-4012-84c1-d5ac1dae0411.png"/></a></p>
<p>If you only have one graphics card with no more than 12G of video memory and need to use it for Windows, please close all programs or web pages that consume video memory to reduce additional memory consumption before training.</p>
<p>diffusers cannot be trained directly using ckpt files, they need to be converted first, as shown in the example below.</p>
<div class="highlight"><pre><span></span><code>    python diffusers<span class="se">\s</span>cripts<span class="se">\c</span>onvert_original_stable_diffusion_to_diffusers.py  --checkpoint_path model.ckpt  --original_config_file v1-inference.yaml  --scheduler_type ddim  --dump_path models/diffusers_model
</code></pre></div>
<p>The training is also packaged and converted to ckpt for use in the WebUI of AUTOMATIC1111.</p>
<div class="highlight"><pre><span></span><code>    python diffusers\scripts\convert_diffusers_to_original_stable_diffusion.py  --model_path models/resultModel  --checkpoint_path result.ckpt  --half
</code></pre></div>
<blockquote>
<p>This is a temporary solution.</p>
</blockquote>
<h2 id="other">Other?</h2>
<p><a href="https://github.com/TheLastBen/fast-stable-diffusion">fast-stable-diffusion colabs</a></p>
<p><a href="https://colab.research.google.com/github/ShivamShrirao/diffusers/blob/main/examples/dreambooth/DreamBooth_Stable_Diffusion.ipynb#scrollTo=K6xoHWSsbcS3">DreamBooth_Stable_Diffusion</a> </p>
<p><a href="https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast-DreamBooth.ipynb">fast-DreamBooth</a></p>
<p><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/2927">Dreambooth Gui</a></p>
<p>When the training model is overfitted, the CFG is very influential, so try low the CFG.</p>
<h2 id="dreambooth-parameter-table">DreamBooth Parameter table</h2>
<div class="highlight"><pre><span></span><code>usage: argmark <span class="o">[</span>-h<span class="o">]</span> --pretrained_model_name_or_path
               PRETRAINED_MODEL_NAME_OR_PATH <span class="o">[</span>--revision REVISION<span class="o">]</span>
               <span class="o">[</span>--tokenizer_name TOKENIZER_NAME<span class="o">]</span> --instance_data_dir
               INSTANCE_DATA_DIR <span class="o">[</span>--class_data_dir CLASS_DATA_DIR<span class="o">]</span>
               <span class="o">[</span>--instance_prompt INSTANCE_PROMPT<span class="o">]</span>
               <span class="o">[</span>--class_prompt CLASS_PROMPT<span class="o">]</span>
               <span class="o">[</span>--with_prior_preservation<span class="o">]</span>
               <span class="o">[</span>--prior_loss_weight PRIOR_LOSS_WEIGHT<span class="o">]</span>
               <span class="o">[</span>--num_class_images NUM_CLASS_IMAGES<span class="o">]</span>
               <span class="o">[</span>--output_dir OUTPUT_DIR<span class="o">]</span> <span class="o">[</span>--seed SEED<span class="o">]</span>
               <span class="o">[</span>--resolution RESOLUTION<span class="o">]</span> <span class="o">[</span>--center_crop<span class="o">]</span>
               <span class="o">[</span>--use_filename_as_label<span class="o">]</span> <span class="o">[</span>--use_txt_as_label<span class="o">]</span>
               <span class="o">[</span>--train_text_encoder<span class="o">]</span>
               <span class="o">[</span>--train_batch_size TRAIN_BATCH_SIZE<span class="o">]</span>
               <span class="o">[</span>--sample_batch_size SAMPLE_BATCH_SIZE<span class="o">]</span>
               <span class="o">[</span>--num_train_epochs NUM_TRAIN_EPOCHS<span class="o">]</span>
               <span class="o">[</span>--max_train_steps MAX_TRAIN_STEPS<span class="o">]</span>
               <span class="o">[</span>--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS<span class="o">]</span>
               <span class="o">[</span>--gradient_checkpointing<span class="o">]</span>
               <span class="o">[</span>--learning_rate LEARNING_RATE<span class="o">]</span> <span class="o">[</span>--scale_lr<span class="o">]</span>
               <span class="o">[</span>--lr_scheduler LR_SCHEDULER<span class="o">]</span>
               <span class="o">[</span>--lr_warmup_steps LR_WARMUP_STEPS<span class="o">]</span> <span class="o">[</span>--use_8bit_adam<span class="o">]</span>
               <span class="o">[</span>--adam_beta1 ADAM_BETA1<span class="o">]</span> <span class="o">[</span>--adam_beta2 ADAM_BETA2<span class="o">]</span>
               <span class="o">[</span>--adam_weight_decay ADAM_WEIGHT_DECAY<span class="o">]</span>
               <span class="o">[</span>--adam_epsilon ADAM_EPSILON<span class="o">]</span>
               <span class="o">[</span>--max_grad_norm MAX_GRAD_NORM<span class="o">]</span> <span class="o">[</span>--push_to_hub<span class="o">]</span>
               <span class="o">[</span>--hub_token HUB_TOKEN<span class="o">]</span> <span class="o">[</span>--hub_model_id HUB_MODEL_ID<span class="o">]</span>
               <span class="o">[</span>--logging_dir LOGGING_DIR<span class="o">]</span>
               <span class="o">[</span>--log_with <span class="o">{</span>tensorboard,wandb<span class="o">}]</span>
               <span class="o">[</span>--mixed_precision <span class="o">{</span>no,fp16,bf16<span class="o">}]</span>
               <span class="o">[</span>--local_rank LOCAL_RANK<span class="o">]</span>
               <span class="o">[</span>--save_model_every_n_steps SAVE_MODEL_EVERY_N_STEPS<span class="o">]</span>
               <span class="o">[</span>--auto_test_model<span class="o">]</span> <span class="o">[</span>--test_prompt TEST_PROMPT<span class="o">]</span>
               <span class="o">[</span>--test_prompts_file TEST_PROMPTS_FILE<span class="o">]</span>
               <span class="o">[</span>--test_negative_prompt TEST_NEGATIVE_PROMPT<span class="o">]</span>
               <span class="o">[</span>--test_seed TEST_SEED<span class="o">]</span>
               <span class="o">[</span>--test_num_per_prompt TEST_NUM_PER_PROMPT<span class="o">]</span>
</code></pre></div>
<h3 id="arguments">Arguments</h3>
<table>
<thead>
<tr>
<th align="left">short</th>
<th align="left">long</th>
<th align="left">default</th>
<th align="left">help</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>-h</code></td>
<td align="left"><code>--help</code></td>
<td align="left"></td>
<td align="left">show this help message and exit</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--pretrained_model_name_or_path</code></td>
<td align="left"><code>None</code></td>
<td align="left">Path to pretrained model or model identifier from huggingface.co/models.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--revision</code></td>
<td align="left"><code>None</code></td>
<td align="left">Revision of pretrained model identifier from huggingface.co/models.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--tokenizer_name</code></td>
<td align="left"><code>None</code></td>
<td align="left">Pretrained tokenizer name or path if not the same as model_name</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--instance_data_dir</code></td>
<td align="left"><code>None</code></td>
<td align="left">A folder containing the training data of instance images.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--class_data_dir</code></td>
<td align="left"><code>None</code></td>
<td align="left">A folder containing the training data of class images.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--instance_prompt</code></td>
<td align="left"><code>None</code></td>
<td align="left">The prompt with identifier specifying the instance</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--class_prompt</code></td>
<td align="left"><code>None</code></td>
<td align="left">The prompt to specify images in the same class as provided instance images.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--with_prior_preservation</code></td>
<td align="left"></td>
<td align="left">Flag to add prior preservation loss.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--prior_loss_weight</code></td>
<td align="left"><code>1.0</code></td>
<td align="left">The weight of prior preservation loss.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--num_class_images</code></td>
<td align="left"><code>100</code></td>
<td align="left">Minimal class images for prior preservation loss. If not have enough images, additional images will be sampled with class_prompt.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--output_dir</code></td>
<td align="left"><code>text-inversion-model</code></td>
<td align="left">The output directory where the model predictions and checkpoints will be written.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--seed</code></td>
<td align="left"><code>None</code></td>
<td align="left">A seed for reproducible training.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--resolution</code></td>
<td align="left"><code>512</code></td>
<td align="left">The resolution for input images, all the images in the train/validation dataset will be resized to this resolution</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--center_crop</code></td>
<td align="left"></td>
<td align="left">Whether to center crop images before resizing to resolution</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--use_filename_as_label</code></td>
<td align="left"></td>
<td align="left">Uses the filename as the image labels instead of the instance_prompt, useful for regularization when training for styles with wide image variance</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--use_txt_as_label</code></td>
<td align="left"></td>
<td align="left">Uses the filename.txt file's content as the image labels instead of the instance_prompt, useful for regularization when training for styles with wide image variance</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--train_text_encoder</code></td>
<td align="left"></td>
<td align="left">Whether to train the text encoder</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--train_batch_size</code></td>
<td align="left"><code>4</code></td>
<td align="left">Batch size (per device) for the training dataloader.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--sample_batch_size</code></td>
<td align="left"><code>4</code></td>
<td align="left">Batch size (per device) for sampling images.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--num_train_epochs</code></td>
<td align="left"><code>1</code></td>
<td align="left"><code>None</code></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--max_train_steps</code></td>
<td align="left"><code>None</code></td>
<td align="left">Total number of training steps to perform.  If provided, overrides num_train_epochs.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--gradient_accumulation_steps</code></td>
<td align="left"><code>1</code></td>
<td align="left">Number of updates steps to accumulate before performing a backward/update pass.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--gradient_checkpointing</code></td>
<td align="left"></td>
<td align="left">Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--learning_rate</code></td>
<td align="left"><code>5e-06</code></td>
<td align="left">Initial learning rate (after the potential warmup period) to use.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--scale_lr</code></td>
<td align="left"></td>
<td align="left">Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--lr_scheduler</code></td>
<td align="left"><code>constant</code></td>
<td align="left">The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--lr_warmup_steps</code></td>
<td align="left"><code>500</code></td>
<td align="left">Number of steps for the warmup in the lr scheduler.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--use_8bit_adam</code></td>
<td align="left"></td>
<td align="left">Whether or not to use 8-bit Adam from bitsandbytes.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--adam_beta1</code></td>
<td align="left"><code>0.9</code></td>
<td align="left">The beta1 parameter for the Adam optimizer.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--adam_beta2</code></td>
<td align="left"><code>0.999</code></td>
<td align="left">The beta2 parameter for the Adam optimizer.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--adam_weight_decay</code></td>
<td align="left"><code>0.01</code></td>
<td align="left">Weight decay to use.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--adam_epsilon</code></td>
<td align="left"><code>1e-08</code></td>
<td align="left">Epsilon value for the Adam optimizer</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--max_grad_norm</code></td>
<td align="left"><code>1.0</code></td>
<td align="left">Max gradient norm.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--push_to_hub</code></td>
<td align="left"></td>
<td align="left">Whether or not to push the model to the Hub.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--hub_token</code></td>
<td align="left"><code>None</code></td>
<td align="left">The token to use to push to the Model Hub.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--hub_model_id</code></td>
<td align="left"><code>None</code></td>
<td align="left">The name of the repository to keep in sync with the local <code>output_dir</code>.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--logging_dir</code></td>
<td align="left"><code>logs</code></td>
<td align="left"><a href="https://www.tensorflow.org/tensorboard">TensorBoard</a> log directory. Will default to <em>output_dir/runs/</em> <em>CURRENT_DATETIME_HOSTNAME</em>**.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--log_with</code></td>
<td align="left"><code>tensorboard</code></td>
<td align="left"><code>None</code></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--mixed_precision</code></td>
<td align="left"><code>no</code></td>
<td align="left">Whether to use mixed precision. Choosebetween fp16 and bf16 (bfloat16). Bf16 requires PyTorch &gt;= 1.10.and an Nvidia Ampere GPU.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--local_rank</code></td>
<td align="left"><code>-1</code></td>
<td align="left">For distributed training: local_rank</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--save_model_every_n_steps</code></td>
<td align="left"><code>None</code></td>
<td align="left"><code>None</code></td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--auto_test_model</code></td>
<td align="left"></td>
<td align="left">Whether or not to automatically test the model after saving it</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--test_prompt</code></td>
<td align="left"><code>A photo of a cat</code></td>
<td align="left">The prompt to use for testing the model.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--test_prompts_file</code></td>
<td align="left"><code>None</code></td>
<td align="left">The file containing the prompts to use for testing the model.example: test_prompts.txt, each line is a prompt</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--test_negative_prompt</code></td>
<td align="left">``</td>
<td align="left">The negative prompt to use for testing the model.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--test_seed</code></td>
<td align="left"><code>42</code></td>
<td align="left">The seed to use for testing the model.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>--test_num_per_prompt</code></td>
<td align="left"><code>1</code></td>
<td align="left">The number of images to generate per prompt.</td>
</tr>
</tbody>
</table>
<ul>
<li>细节</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">instance_data_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"You must specify a train data directory."</span><span class="p">)</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">with_prior_preservation</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">class_data_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"You must specify a data directory for class images."</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">class_prompt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"You must specify prompt for class images."</span><span class="p">)</span>
</code></pre></div>
<!--
[飞桨 dreambooth 训练教程](https://docs.qq.com/doc/DUHVuZ3BNV0FkT1R6)
-->
<p>[6^]:<a href="https://zhuanlan.zhihu.com/p/584736850">StableDiffusion/DreamBooth 自训练全教程</a></p>
<hr/>
<div class="md-source-file">
<small>
    
      Last update:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime">November 22, 2022 16:22:17</span>
<br/>
        Created:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime">November 3, 2022 12:19:59</span>
</small>
</div>
</article>
</div>
</div>
<a class="md-top md-icon" data-md-component="top" hidden="" href="#">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
            Back to top
          </a>
</main>
<footer class="md-footer">
<nav aria-label="Footer" class="md-footer__inner md-grid">
<a aria-label="Previous: Hypernetwork" class="md-footer__link md-footer__link--prev" href="../Hypernetwork/" rel="prev">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</div>
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                Previous
              </span>
              Hypernetwork
            </div>
</div>
</a>
<a aria-label="Next: AestheticGradients" class="md-footer__link md-footer__link--next" href="../AestheticGradients/" rel="next">
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                Next
              </span>
              AestheticGradients
            </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
      Copyright © 采用 GFDL 协议

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://t.me/s/StableDiffusion_CN_WIKI" rel="noopener" target="_blank" title="t.me">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M248 8C111.033 8 0 119.033 0 256s111.033 248 248 248 248-111.033 248-248S384.967 8 248 8Zm114.952 168.66c-3.732 39.215-19.881 134.378-28.1 178.3-3.476 18.584-10.322 24.816-16.948 25.425-14.4 1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25 5.342-39.5 3.652-3.793 67.107-61.51 68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608 69.142-14.845 10.194-26.894 9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7 18.45-13.7 108.446-47.248 144.628-62.3c68.872-28.647 83.183-33.623 92.511-33.789 2.052-.034 6.639.474 9.61 2.885a10.452 10.452 0 0 1 3.53 6.716 43.765 43.765 0 0 1 .417 9.769Z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "toc.follow", "navigation.top", "navigation.tracking", "navigation.indexes", "navigation.expand", "navigation.sections"], "search": "../../../assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
<script src="../../../assets/javascripts/bundle.d6c3db9e.min.js"></script>
<script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body>
</html>